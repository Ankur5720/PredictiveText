{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictiveText.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AHfE37uVgAd"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eR1X1oRXM1y"
      },
      "source": [
        "def load_doc(filename):\n",
        "  file=open(filename,'r')\n",
        "  text=file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IXp4iSWaCEG"
      },
      "source": [
        "in_filename='/content/drive/MyDrive/alice.txt'\n",
        "doc=load_doc(in_filename)\n",
        "print(doc[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VC6iBkkaZa4"
      },
      "source": [
        "import string\n",
        "def clean_doc(doc):\n",
        "  doc=doc.replace('--',' ')\n",
        "  tokens=doc.split()\n",
        "  table=str.maketrans('','',string.punctuation)\n",
        "  tokens=[w.translate(table) for w in tokens]\n",
        "  tokens=[word for word in tokens if word.isalpha()]\n",
        "  tokens=[word.lower() for word in tokens]\n",
        "  return tokens"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slz-EZW1a1Qe"
      },
      "source": [
        "tokens=clean_doc(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njp5mvvvcJa7"
      },
      "source": [
        "length=50+1\n",
        "sequences=list()\n",
        "for i in range(length,len(tokens)):\n",
        "  seq=tokens[i-length:i]\n",
        "  line=' '.join(seq)\n",
        "  sequences.append(line)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciB5NS97dqrw"
      },
      "source": [
        "def save_doc(lines,filename):\n",
        "  data='\\n'.join(lines)\n",
        "  file=open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSpawcWddbcJ"
      },
      "source": [
        "out_filename='/content/drive/MyDrive/alice_sequences.txt'\n",
        "save_doc(sequences,out_filename)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbVet_xyd_0z"
      },
      "source": [
        "doc1=load_doc(out_filename)\n",
        "lines=doc1.split('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdmJu2Vqe9e4"
      },
      "source": [
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences=tokenizer.texts_to_sequences(lines)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FEvCKachfz7"
      },
      "source": [
        "vocab_size=len(tokenizer.word_index)+1\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uj7b1ROk4RB"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "sequences1= np.array(sequences)\n",
        "X, y = sequences1[:,:-1], sequences1[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n034gGdIn5q4",
        "outputId": "fe16cddf-e780-4335-a6c9-9f55f68357ae"
      },
      "source": [
        "# Embedding layer accepts input as(input_dim,output_dim,input_length)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=10)\n",
        " "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            154350    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3087)              311787    \n",
            "=================================================================\n",
            "Total params: 617,037\n",
            "Trainable params: 617,037\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "230/230 [==============================] - 52s 213ms/step - loss: 6.4376 - accuracy: 0.0608\n",
            "Epoch 2/10\n",
            "230/230 [==============================] - 49s 213ms/step - loss: 6.0527 - accuracy: 0.0623\n",
            "Epoch 3/10\n",
            "230/230 [==============================] - 49s 212ms/step - loss: 5.8527 - accuracy: 0.0657\n",
            "Epoch 4/10\n",
            "230/230 [==============================] - 48s 210ms/step - loss: 5.7385 - accuracy: 0.0705\n",
            "Epoch 5/10\n",
            "230/230 [==============================] - 49s 212ms/step - loss: 5.6489 - accuracy: 0.0755\n",
            "Epoch 6/10\n",
            "230/230 [==============================] - 48s 208ms/step - loss: 5.5425 - accuracy: 0.0813\n",
            "Epoch 7/10\n",
            "230/230 [==============================] - 47s 204ms/step - loss: 5.4300 - accuracy: 0.0926\n",
            "Epoch 8/10\n",
            "230/230 [==============================] - 47s 205ms/step - loss: 5.3277 - accuracy: 0.1006\n",
            "Epoch 9/10\n",
            "230/230 [==============================] - 47s 206ms/step - loss: 5.2397 - accuracy: 0.1047\n",
            "Epoch 10/10\n",
            "230/230 [==============================] - 47s 204ms/step - loss: 5.1602 - accuracy: 0.1117\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f62b9805310>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MegH_kCbq0p_"
      },
      "source": [
        "\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from pickle import dump"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2Dar757olpH"
      },
      "source": [
        "model.save('/content/drive/MyDrive/model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('/content/drive/MyDrive/tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0oCVv4Qx4vP"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/drive/MyDrive/tokenizer.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFyEaFDrq7SA"
      },
      "source": [
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  # generate a fixed number of words\n",
        "  for _ in range(n_words):\n",
        "      # encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    # truncate sequences to a fixed length\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "    # predict probabilities for each word\n",
        "    yhat = model.predict(encoded, verbose=0)\n",
        "    # print(yhat,type(yhat),len(yhat))\n",
        "    yhat=np.argmax(yhat,axis=1)\n",
        "    # print(yhat)\n",
        "    # map predicted word index to word\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    # append to input\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)\n",
        " \n",
        "seq_length = len(lines[0].split()) - 1\n",
        "model = load_model('/content/drive/MyDrive/model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/drive/MyDrive/tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "# print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4rp4j8Yxu37"
      },
      "source": [
        "seq_length = len(lines[0].split()) - 1\n",
        "model = load_model('/content/drive/MyDrive/model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/drive/MyDrive/tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "# print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omK2jdzzzist",
        "outputId": "5ad2eafa-bf3d-4a42-90dc-26d3a3a0e940"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from numpy import array\n",
        "# define model\n",
        "inputs1 = Input(shape=(3, 1))\n",
        "lstm1 = LSTM(1, return_sequences=True)(inputs1)\n",
        "model = Model(inputs=inputs1, outputs=lstm1)\n",
        "# define input data\n",
        "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))\n",
        "# make and show prediction\n",
        "print(model.predict(data))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.01474419]\n",
            "  [0.04063214]\n",
            "  [0.07437418]]]\n"
          ]
        }
      ]
    }
  ]
}